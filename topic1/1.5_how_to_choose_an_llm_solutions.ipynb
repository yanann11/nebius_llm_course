{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanann11/nebius_llm_course/blob/main/topic1/1.5_how_to_choose_an_llm_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon.\n",
        "\n",
        "# 1.5. How to choose an LLM"
      ],
      "metadata": {
        "id": "Vm506vpf9u9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice solutions"
      ],
      "metadata": {
        "id": "9xwnPidiSx1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. Advanced MMLU testing\n",
        "\n",
        "In this task, you'll need to upgrade the `MMLUEvaluator` class to also compare:\n",
        "\n",
        "1. **Average latency** (that is, average time to solve a problem). Add `'avg_inference_time'` to the outputs of `run_evaluation`. Make sure that you only measure the timing of producing the competion, not of the whole `evaluate_single_question` running - this will be especially relevant when we add the translation phase.\n",
        "\n",
        "  In theory, average latency would reflect the LLM's size and average answer length. Note that for rarer languages tokens will be smaller, and as consequence the answer length in tokens will be larger (even if visible answer length will be comparable with English). This will, of course, contribute to the latency.\n",
        "  \n",
        "  In reality though, average latency also highly depends on the *API provider* or your own deployment efforts. APIs may have periods of higher or lower latency; they also introduce optimizations which might work or not work, depending on the architectural details of different LLMs.\n",
        "\n",
        "2. **Multilingual proficiency**. Almost every Q&A-related benchmark exposes LLMs to questions in English, because\n",
        "\n",
        "  (a) gathering data in English is much easier than in any other language,\n",
        "\n",
        "  (b) English benchmarks are relevant to larger portion of the AI community,\n",
        "\n",
        "  (c) the numbers look better when you check things in English :)\n",
        "\n",
        "  But in this task you'll try to add a `language` parameter to the `run_evaluation` mehtod. When it's `None`, the LLM will be tested on the original English questions and answers; otherwise, the specified language will be used. If you have time, try several MMLU topics and several languages. How much will the quality fall in comparison with English?\n",
        "\n",
        "  You'll need to use an LLM for translation of questions and answers. Some guidelines you might have in mind:\n",
        "\n",
        "  * Choose the translator LLM wisely. We suggest using a powerful one, because otherwise you'll see the effects of translation, not of the language choice. If you have access to OpenAI or Anthropic API, leveraging their models won't hurt. If you use long-reasoning models such as `o4`, `DeepSeek R1`, or `Qwen3`, don't forget to increase the `max_tokens` parameter for the translator call, because these models tend to be wordy.\n",
        "  * Assess the translation quality before you start running your benchmarks. For that, choose the language you or your friends know well.\n",
        "  * You might want to cache the translations if you're going to test multiple LLMs.\n",
        "  * Generally, there are two strategies of translation. You can either feed the whole\n",
        "\n",
        "    ```\n",
        "    QUESTION: {question}\n",
        "\n",
        "    ANSWER OPTIONS:\n",
        "    A: {A}\n",
        "    B: {B}\n",
        "    C: {C}\n",
        "    D: {D}\n",
        "    ```\n",
        "  \n",
        "    structure to the translator or translate the question and the answer options separately. The second option will be slightly more expensive. The first one might be tricky, because you'll need the LLM to strictly obey the format and abstain from commenting on the answers or a potential solution. It can be achieved through clever prompting, but the better strategy is using either few-shot examples or structured generation which will be the discussed in Topic 2. So, for now, we suggest separate translation."
      ],
      "metadata": {
        "id": "Sgxa3TQvSzZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**. Here's out implementation. It translates questions and answers separately, while providing an option of not translating answers (which is perfectly reasonable for math questions). It also caches translations. For demonstrational purposes we use **Llama-3.1-405B** as a translator model."
      ],
      "metadata": {
        "id": "aW_-cmo7oBAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhpPRCwzT7-I",
        "outputId": "c9d15b9f-c47b-47ca-8dd7-fe59868e0040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/491.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import hashlib\n",
        "import pickle\n",
        "from openai import OpenAI\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\",\n",
        "                 translator_client=nebius_client, translator_model=None,\n",
        "                 cache_dir: str = \"translation_cache\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "            translator_client: Client for translation model\n",
        "            translator_model: Model to use for translation\n",
        "            cache_dir: Directory to store translation cache\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "        self.translator_client = translator_client\n",
        "        self.translator_model = translator_model\n",
        "\n",
        "        # Setup cache directory and translation cache\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
        "        self.translation_cache = self._load_translation_cache()\n",
        "\n",
        "        # Simple translation prompt for a single text\n",
        "        self.simple_translation_prompt = \"\"\"\n",
        "Translate the following text from English to {language}.\n",
        "Return only the translated content enclosed within <translation> and </translation> tags.\n",
        "Leave formulas and mathematical notations as they are.\n",
        "\n",
        "Text to translate: {text}\n",
        "\"\"\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def _get_cache_key(self, text: str, language: str, model: Optional[str] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a unique cache key for a text and language combination.\n",
        "\n",
        "        Args:\n",
        "            text: Text to translate\n",
        "            language: Target language\n",
        "            model: Optional model identifier\n",
        "\n",
        "        Returns:\n",
        "            A unique hash key for the translation\n",
        "        \"\"\"\n",
        "        model_str = model or self.translator_model or \"default_model\"\n",
        "        content = f\"{text}_{language}_{model_str}\"\n",
        "        return hashlib.md5(content.encode()).hexdigest()\n",
        "\n",
        "    def _get_cache_path(self) -> Path:\n",
        "        \"\"\"Get the path to the translation cache file.\"\"\"\n",
        "        topic_safe = self.topic.replace(\"/\", \"_\")\n",
        "        return self.cache_dir / f\"{topic_safe}_translation_cache.pkl\"\n",
        "\n",
        "    def _load_translation_cache(self) -> Dict:\n",
        "        \"\"\"Load the translation cache from disk if it exists.\"\"\"\n",
        "        cache_path = self._get_cache_path()\n",
        "        if cache_path.exists():\n",
        "            try:\n",
        "                with open(cache_path, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading translation cache: {e}\")\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def _save_translation_cache(self):\n",
        "        \"\"\"Save the translation cache to disk.\"\"\"\n",
        "        cache_path = self._get_cache_path()\n",
        "        try:\n",
        "            with open(cache_path, 'wb') as f:\n",
        "                pickle.dump(self.translation_cache, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving translation cache: {e}\")\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def translate_text(self, text: str, language: str) -> Tuple[str, str]:\n",
        "        \"\"\"\n",
        "        Translate a single piece of text to the target language.\n",
        "        Uses cache if available, otherwise calls the translation model.\n",
        "\n",
        "        Args:\n",
        "            text: The text to translate\n",
        "            language: Target language for translation\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (translated_text, raw_response) or (original_text, error_message) if translation fails\n",
        "        \"\"\"\n",
        "        if not language or not self.translator_client or not self.translator_model:\n",
        "            return text, \"No translation requested\"\n",
        "\n",
        "        # Generate cache key\n",
        "        cache_key = self._get_cache_key(text, language, self.translator_model)\n",
        "\n",
        "        # Check if we have this translation in cache\n",
        "        if cache_key in self.translation_cache:\n",
        "            cached_result = self.translation_cache[cache_key]\n",
        "            print(f\"Using cached translation for: {text[:30]}...\")\n",
        "            return cached_result[\"translation\"], cached_result[\"raw_response\"]\n",
        "\n",
        "        # Not in cache, perform translation\n",
        "        try:\n",
        "            translation_prompt = self.simple_translation_prompt.format(\n",
        "                language=language,\n",
        "                text=text\n",
        "            )\n",
        "\n",
        "            translation_response = answer_with_llm(\n",
        "                prompt=translation_prompt,\n",
        "                system_prompt=f\"You are a professional translator from English to {language}.\",\n",
        "                client=self.translator_client,\n",
        "                model=self.translator_model,\n",
        "                prettify=False\n",
        "            )\n",
        "\n",
        "            # Extract translation from between tags\n",
        "            try:\n",
        "                translation = translation_response.split('<translation>')[1].split('</translation>')[0].strip()\n",
        "\n",
        "                # Save to cache\n",
        "                self.translation_cache[cache_key] = {\n",
        "                    \"translation\": translation,\n",
        "                    \"raw_response\": translation_response,\n",
        "                    \"timestamp\": time.time()\n",
        "                }\n",
        "\n",
        "                # Persist cache to disk\n",
        "                self._save_translation_cache()\n",
        "\n",
        "                return translation, translation_response\n",
        "            except:\n",
        "                error_msg = f\"Failed to extract translation for: {text[:30]}...\"\n",
        "                print(error_msg)\n",
        "                return text, f\"{error_msg}\\nRaw response: {translation_response}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Translation error: {e}\"\n",
        "            print(error_msg)\n",
        "            return text, error_msg\n",
        "\n",
        "    def translate_problem(self, question: str, choices: Dict[str, str],\n",
        "                          language: str, translate_answers: bool = True) -> Tuple[str, Dict[str, str], Dict]:\n",
        "        \"\"\"\n",
        "        Translate the problem to the target language.\n",
        "\n",
        "        Args:\n",
        "            question: The question to translate\n",
        "            choices: The answer choices to translate\n",
        "            language: Target language for translation\n",
        "            translate_answers: Whether to translate answer options\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (translated_question, translated_choices, translation_logs)\n",
        "        \"\"\"\n",
        "        translation_logs = {\n",
        "            \"question\": {\n",
        "                \"original\": question,\n",
        "                \"translated\": None,\n",
        "                \"raw_response\": None\n",
        "            },\n",
        "            \"choices\": {}\n",
        "        }\n",
        "\n",
        "        if not language or not self.translator_client or not self.translator_model:\n",
        "            return question, choices, translation_logs\n",
        "\n",
        "        # Translate the question\n",
        "        translated_question, raw_response = self.translate_text(question, language)\n",
        "        translation_logs[\"question\"][\"translated\"] = translated_question\n",
        "        translation_logs[\"question\"][\"raw_response\"] = raw_response\n",
        "\n",
        "        # If we're not translating answers, return just the translated question\n",
        "        if not translate_answers:\n",
        "            for key, value in choices.items():\n",
        "                translation_logs[\"choices\"][key] = {\n",
        "                    \"original\": value,\n",
        "                    \"translated\": value,  # Not translated\n",
        "                    \"raw_response\": \"Answer translation disabled\"\n",
        "                }\n",
        "            return translated_question, choices, translation_logs\n",
        "\n",
        "        # Translate each answer option individually\n",
        "        translated_choices = {}\n",
        "        for key, value in choices.items():\n",
        "            translated_choice, raw_response = self.translate_text(value, language)\n",
        "            translated_choices[key] = translated_choice\n",
        "\n",
        "            # Log the translation\n",
        "            translation_logs[\"choices\"][key] = {\n",
        "                \"original\": value,\n",
        "                \"translated\": translated_choice,\n",
        "                \"raw_response\": raw_response\n",
        "            }\n",
        "\n",
        "        return translated_question, translated_choices, translation_logs\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model, language=None,\n",
        "                                 translate_answers=True) -> Tuple[bool, str, str, float, Dict]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "            language: Target language for translation (None for English)\n",
        "            translate_answers: Whether to translate answer options (default: True)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response, inference_time, translation_logs)\n",
        "        \"\"\"\n",
        "        translation_logs = None\n",
        "\n",
        "        try:\n",
        "            # Translate if needed\n",
        "            if language:\n",
        "                translated_question, translated_choices, translation_logs = self.translate_problem(\n",
        "                    question, choices, language, translate_answers\n",
        "                )\n",
        "                # Use translated content\n",
        "                question = translated_question\n",
        "                choices = translated_choices\n",
        "\n",
        "            formatted_prompt = self.prompt.format(\n",
        "                topic_prettified=self.topic_prettified,\n",
        "                question=question,\n",
        "                A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "            )\n",
        "\n",
        "            # Measure inference time\n",
        "            start_time = time.time()\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=formatted_prompt,\n",
        "                system_prompt=self.system_prompt,\n",
        "                client=client,\n",
        "                model=model,\n",
        "                prettify=False\n",
        "            )\n",
        "            end_time = time.time()\n",
        "            inference_time = end_time - start_time\n",
        "\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response, inference_time, translation_logs\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None, 0, translation_logs\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=None,\n",
        "                       n_questions=50, language=None,\n",
        "                       translate_answers=True) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "            language: Target language for translation (None for English)\n",
        "            translate_answers: Whether to translate answer options (default: True)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "        total_time = 0\n",
        "        translation_logs = []\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response, inference_time, trans_log = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "                language=language,\n",
        "                translate_answers=translate_answers\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            total_time += inference_time\n",
        "\n",
        "            log_entry = {\n",
        "                'question_id': i,\n",
        "                'original_question': self.questions[i],\n",
        "                'original_choices': self.choices.iloc[i].to_dict(),\n",
        "                'correct_answer': self.answers[i],\n",
        "                'model_answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct,\n",
        "                'inference_time': inference_time,\n",
        "                'translation_log': trans_log\n",
        "            }\n",
        "\n",
        "            evaluation_log.append(log_entry)\n",
        "\n",
        "            # Add to translation logs if available\n",
        "            if trans_log:\n",
        "                translation_logs.append({\n",
        "                    'question_id': i,\n",
        "                    'translation_log': trans_log\n",
        "                })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        avg_inference_time = total_time / n_questions if n_questions > 0 else 0\n",
        "\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'avg_inference_time': avg_inference_time,\n",
        "            'total_inference_time': total_time,\n",
        "            'evaluation_log': evaluation_log,\n",
        "            'translation_logs': translation_logs,\n",
        "            'language': language,\n",
        "            'translate_answers': translate_answers,\n",
        "            'cache_stats': {\n",
        "                'cache_size': len(self.translation_cache),\n",
        "                'cache_path': str(self._get_cache_path())\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n",
        "\n",
        "    def clear_translation_cache(self):\n",
        "        \"\"\"Clear the translation cache and delete the cache file.\"\"\"\n",
        "        self.translation_cache = {}\n",
        "        cache_path = self._get_cache_path()\n",
        "        if cache_path.exists():\n",
        "            cache_path.unlink()\n",
        "            print(f\"Deleted translation cache file: {cache_path}\")"
      ],
      "metadata": {
        "id": "RSUu3GnZS0zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsJ5IAKRTvqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MMLUEvaluator(\n",
        "    topic=\"high_school_mathematics\", translator_model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "    )"
      ],
      "metadata": {
        "id": "FLLOQZ6qUHWP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432,
          "referenced_widgets": [
            "9d433955c4834915884881ea1d21d355",
            "1fb34a097fff4cb789d16103c6bc6f1d",
            "0ffa5c2006534d7491fac4c2793e6f22",
            "d775ab45170d486e944b1271805cd6d0",
            "b8d419be71584f9b9a8a53f8d6ba4321",
            "c5ef67ae8069414c98b949bc61184509",
            "91b0c9c69314438392b5bd303d18afad",
            "bdfe86acf7ad4b2fac16eaf77f74eff6",
            "d540898a0b254b1586df31ec4c153f22",
            "465dd55cbd0741fdb88e9537ed61e5a9",
            "c3179689eeb542cd86dba4d623b6f18b",
            "180a8a331c0d436d8ddbee451817b1a0",
            "3c4aae78d76d452080f8a3e7a2cb6520",
            "a1936e28ebc24dfc957fba46f00dab61",
            "426284d96a104a1cbf8dbb0e019aaef4",
            "7041d86ce1df4e518540dc6855271139",
            "1720e03719974dfbb85b824b79906027",
            "8dda42314ea348ff84c5ae7d111e02b0",
            "65d1bde3b86e4a88a8e54b7bac3b686f",
            "a0d09ea3736a406e94bd942f3cea858d",
            "c1ba32d491c24f94b48d8806e084e584",
            "52ae9b64e9dd404da167e805588c1d0a",
            "226b33fadcf34aecaf8877a4a379f0d4",
            "a2bcbd5da5524dff9bd90cc89b1ce863",
            "4c5a9275e25e4d42b54d72755e32854a",
            "279fee6ae6db41ec8c2cba7174e04c46",
            "ef6c2f2a9579413c90c69a7f7b79ea10",
            "577154835073490ca779036091c66f86",
            "2b56421217744422989acd996639d255",
            "40c9d9f513d44417bbfcab7c9985104b",
            "51fb32c541eb4449bc613525ec2a99c5",
            "bb12366bee2b42ac8dff5aa7cd487cb5",
            "aae48aaeb50349a28f4169641e05f334",
            "377313e993944f91bb7122f953971031",
            "568771c3ba1346b2a7bab3dacb7b8041",
            "0f0036ac79fd46dabe77f6505b651a8c",
            "69b1b8faa29d4af8a030df8a93924e49",
            "09b425383dde4c68a949006576bf8f7f",
            "46b1d02c57a744c6989c0beebf8db22d",
            "7163d09489134382a2b070d73679d778",
            "3383c3d95eb749d9b4f0efb08280afb7",
            "5fbb1befb12b418daba97726765da327",
            "eef30b924693457997acebcfba91fd29",
            "9db40fc7890d465086f86056f80649a5",
            "1bc86c129fab4cc29f93124d73505926",
            "26b56ff228cd4cd49d3c143634329cfb",
            "010f750f7b5943d7aa9d226514e6036b",
            "6deb47d0274746e78604adae55f8013c",
            "5f427f37a0454d5896706c77325957f1",
            "13223df677ea4c5ea1247fb3d03151d3",
            "c9508a3a36eb4875b1b388fe57f99f39",
            "174c7bd925134e0bbe3338975c6be12d",
            "2c4f4ad6d9b6466688f81608e98a2cc6",
            "64569ae11b624c56bb36c3953d447ffa",
            "7d471755092f4ac89b16bdfe16ec4982",
            "532972f013754e08ada03ff85908d63b",
            "9d4de77181f54609a6b6c3baff39a7fa",
            "bb3c3be8fcf4460b981dca6a5251ab8e",
            "f09da9d739b049079edcb90e2b8a007a",
            "3d0e4653f9cf4ada89fcbfcee2d5431b",
            "8af5be900ea748d6b17661f5a2fb6ab2",
            "117bf44d3cc14b4db9496874bbd36285",
            "88f99e55cc1241589adbcbd51050d617",
            "ea963c769f3b4634af275ad42c1392ad",
            "4aa8d45416cb4b7ea4ceaafef653c90d",
            "e065edfc571c44d2910bc6b57b74c5a2",
            "c661461d73cc471691bfc50332f6cc82",
            "f7cdd5231b36403eaca4a8c5eff16fdc",
            "36340a7a7b8f4aa78407781b97065726",
            "4c34d169249c4d3faaa82ed465d31b95",
            "647dd9b46f224fb59c68b5cce7a1cf49",
            "ab04db8b611e4f1293613a1d16f93579",
            "98dc17f2483e4951a712d2537df614ea",
            "4e94658d493f4441b7733c6b95f274bb",
            "8fb1967490d34d2aa313531844753137",
            "bd464c4c6c61406d94739f6207707471",
            "ccb33a3ac0d840f5b0dfdcecd9ae5520",
            "79adf216bb8e437ba920ed06a2904c62",
            "3b54b20869154fca80fca6c45d13d7d7",
            "35d5a3af45154497bc5187577f83a411",
            "3e2dee5f71044cf19bb9f40d39ea086c",
            "9a85c34490aa4360ba3d7425a4551ee9",
            "e8217ea6d6a84e54aa7ee588a703f329",
            "25a4b5378657460a97d5db75f0f76e45",
            "776640a8e9284861baddc4c9b92fb956",
            "03774d2b074f48c2ba691337cb61943c",
            "6d1dfeeacc0c409682ebc826707d1b4c",
            "8b82d31798c448df99f8d75dd3e1edcd"
          ]
        },
        "outputId": "59e54264-7ab8-4449-c09a-e6015d900d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
            "\n",
            "\n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/53.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d433955c4834915884881ea1d21d355"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/138k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "180a8a331c0d436d8ddbee451817b1a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/33.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "226b33fadcf34aecaf8877a4a379f0d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/6.99k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "377313e993944f91bb7122f953971031"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dev-00000-of-00001.parquet:   0%|          | 0.00/4.50k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bc86c129fab4cc29f93124d73505926"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/270 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "532972f013754e08ada03ff85908d63b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/29 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c661461d73cc471691bfc50332f6cc82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79adf216bb8e437ba920ed06a2904c62"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can run evaluation for several models. The function `evaluator.run_evaluation` will return both classification accuracy and the full log containing the model's responses and the extracted answers."
      ],
      "metadata": {
        "id": "BldEeePXUHWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                         n_questions=50, language=None)\n",
        "print(f'\\nAccuracy: {results[\"accuracy\"]}')\n",
        "print(f'\\nAverage inference time: {results[\"avg_inference_time\"]:.1f} sec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdf77f5-34f8-4df1-ce71-26b385bebceb",
        "id": "6eeomGZXUHWQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [07:43<00:00,  9.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.76\n",
            "\n",
            "Average inference time: 9.3 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                         n_questions=50, language=\"French\", translate_answers=False)\n",
        "print(f'\\nAccuracy: {results[\"accuracy\"]}')\n",
        "print(f'\\nAverage inference time: {results[\"avg_inference_time\"]:.1f} sec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NprpLXfZtEU",
        "outputId": "ae4cc8f6-bc2c-485b-8c3b-498ee7c8c200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 38/50 [27:04<05:50, 29.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached translation for: What is the smallest positive ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 48/50 [31:47<00:51, 25.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached translation for: John divided his souvenir hat ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [32:14<00:00, 38.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.34\n",
            "\n",
            "Average inference time: 36.7 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[\"evaluation_log\"][5][\"translation_log\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE012POImKv3",
        "outputId": "f6d743d8-2262-4eab-965e-00467bf69486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': {'original': 'John divided his souvenir hat pins into two piles. The two piles had an equal number of pins. He gave his brother one-half of one-third of one pile. John had 66 pins left. How many pins did John originally have?',\n",
              "  'translated': \"John a divisé ses épingles de chapeau souvenir en deux tas. Les deux tas avaient un nombre égal d'épingles. Il a donné à son frère la moitié du tiers d'un tas. John avait 66 épingles en reste. Combien d'épingles John avait-il à l'origine ?\",\n",
              "  'raw_response': \"<translation>John a divisé ses épingles de chapeau souvenir en deux tas. Les deux tas avaient un nombre égal d'épingles. Il a donné à son frère la moitié du tiers d'un tas. John avait 66 épingles en reste. Combien d'épingles John avait-il à l'origine ?</translation>\"},\n",
              " 'choices': {'A': {'original': '396',\n",
              "   'translated': '396',\n",
              "   'raw_response': 'Answer translation disabled'},\n",
              "  'B': {'original': '72',\n",
              "   'translated': '72',\n",
              "   'raw_response': 'Answer translation disabled'},\n",
              "  'C': {'original': '66',\n",
              "   'translated': '66',\n",
              "   'raw_response': 'Answer translation disabled'},\n",
              "  'D': {'original': '36',\n",
              "   'translated': '36',\n",
              "   'raw_response': 'Answer translation disabled'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}