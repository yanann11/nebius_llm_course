{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanann11/nebius_llm_course/blob/main/topic1/1.2_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon.\n",
        "\n",
        "# 1.2. Tokenization"
      ],
      "metadata": {
        "id": "Vm506vpf9u9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going into the LLM, the prompt is **tokenized**, that is, split into **tokens**. And the completion is also generated **token by token**.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1nKMwN3nZ9ZoEQcdzhxmv5QPXLHv6K7MK\" width=600 />\n",
        "</center>\n",
        "\n",
        "As you see, the words are cut into pieces called **subword units**. Why? This allows to:\n",
        "\n",
        "- Have a dictionary (list of all tokens) of fixes size. This wouldn't work with **word-level** tokeinization: typos and neologisms would make the dictionary too huge.\n",
        "- Have meaningful tokens, unlike in **character-level** tokenization.\n",
        "\n",
        "**Note**. Some LLMs use hybrid strategy, where `import pandas as pd` may be one token."
      ],
      "metadata": {
        "id": "0p-CFy9Q9u9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the OpenAI tokenizer. For that, we need the `tiktoken` library."
      ],
      "metadata": {
        "id": "ih6xPadl9u9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b932b471-40e7-41b5-92e8-f87f6ace6fda",
        "id": "_nULlfhm9u9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "encoding = tiktoken.encoding_for_model('gpt-4o-mini')"
      ],
      "metadata": {
        "id": "sGmJA7Ln9u9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's tokenize a simple prompt."
      ],
      "metadata": {
        "id": "pyEcD4od9u9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_string = 'Darth Vader was born on Tatooine.'\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(initial_string)\n",
        "print('After tokenization (encoded string): ', encoded_string)\n",
        "\n",
        "# Tokenization\n",
        "print('Decoding back: ', encoding.decode(encoded_string))\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string:\n",
        "    print(f'{token}: \"{encoding.decode([token])}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f508aaf-4f42-4463-d71b-2adda6146895",
        "id": "YbwNvJ9O9u9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After tokenization (encoded string):  [35, 7087, 120316, 673, 12275, 402, 353, 2754, 58552, 13]\n",
            "Decoding back:  Darth Vader was born on Tatooine.\n",
            "35: \"D\"\n",
            "7087: \"arth\"\n",
            "120316: \" Vader\"\n",
            "673: \" was\"\n",
            "12275: \" born\"\n",
            "402: \" on\"\n",
            "353: \" T\"\n",
            "2754: \"ato\"\n",
            "58552: \"oine\"\n",
            "13: \".\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We put the tokens in quotes to make it more clear that some tokens have a space before them and some do not. So, `\" D\"` and `\"D\"` will be different tokens; the first one used after a space and the second one in the beginning of a text or in the middle of a word. For example:"
      ],
      "metadata": {
        "id": "k4Epyw1m-hG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_string = 'D D!D'\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(initial_string)\n",
        "print('After tokenization (encoded string): ', encoded_string)\n",
        "\n",
        "# Tokenization\n",
        "print('Decoding back: ', encoding.decode(encoded_string))\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string:\n",
        "    print(f'{token}: \"{encoding.decode([token])}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgjbSu9Y-5e5",
        "outputId": "593e30d9-b7ce-49e4-b381-554c58a0c198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After tokenization (encoded string):  [35, 415, 0, 35]\n",
            "Decoding back:  D D!D\n",
            "35: \"D\"\n",
            "415: \" D\"\n",
            "0: \"!\"\n",
            "35: \"D\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It may also be interesting to see how the model splits code:"
      ],
      "metadata": {
        "id": "dkg9eMye9u9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_code = \"\"\"import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.hist(df['values'], bins=5, edgecolor='black')\n",
        "plt.xlabel('Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Values')\n",
        "plt.show()\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(python_code)\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string[:40]:\n",
        "    print(f'{token}: {encoding.decode([token])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo1wclqG9u9f",
        "outputId": "cda17c1e-a124-4c3f-f6c2-fc3a98445e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "561: import\n",
            "41942:  pandas\n",
            "472:  as\n",
            "15934:  pd\n",
            "198: \n",
            "\n",
            "561: import\n",
            "39480:  matplotlib\n",
            "56679: .pyplot\n",
            "472:  as\n",
            "14248:  plt\n",
            "279: \n",
            "\n",
            "\n",
            "2: #\n",
            "50915:  Plot\n",
            "1744: ting\n",
            "290:  the\n",
            "81482:  histogram\n",
            "198: \n",
            "\n",
            "20940: plt\n",
            "116240: .hist\n",
            "34339: (df\n",
            "1181: ['\n",
            "7222: values\n",
            "9151: '],\n",
            "53960:  bins\n",
            "28: =\n",
            "20: 5\n",
            "11: ,\n",
            "11165:  edge\n",
            "4991: color\n",
            "2053: ='\n",
            "18474: black\n",
            "2515: ')\n",
            "\n",
            "20940: plt\n",
            "81780: .xlabel\n",
            "706: ('\n",
            "8278: Values\n",
            "2515: ')\n",
            "\n",
            "20940: plt\n",
            "81430: .ylabel\n",
            "706: ('\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the tokenizer of an open-source LLM\n",
        "\n",
        "Most open-source LLMs are available at [Hugging Face](https://huggingface.co/). For example, here is the page of the Qwen2.5-Coder-32B model: [link](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct).\n",
        "\n",
        "Usually, a model page contains demo code showcasing how to use this model. Here we only need its tokenizer, so we deleted everything else."
      ],
      "metadata": {
        "id": "xplrLBJF_lwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Darth Vader was born on Tatooine.\"\n",
        "\n",
        "tokenizer.tokenize(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C66qxnLo_jZr",
        "outputId": "a1774380-221e-449f-d2cc-26f101d7bd79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['D', 'arth', 'ĠVader', 'Ġwas', 'Ġborn', 'Ġon', 'ĠTat', 'oo', 'ine', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the tokenization is different from what you've seen for gpt-4o-mini. Also, a symbol `Ġ` is used instead of a space."
      ],
      "metadata": {
        "id": "N3FxqylbBe9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note.** Unlike Qwen, many models will actually ask you to login to Hugging Face and consent to their terms of use before you can get your hands on them.\n",
        "\n",
        "After that, you'll need to:\n",
        "\n",
        "1. [Get an access token](https://huggingface.co/settings/tokens) with **write** permissions. Make sure to copy and save it; you won't be able to see it again.\n",
        "\n",
        "2. Login to Hugging Face from Jupyter, for example, using\n",
        "\n",
        "```\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "```\n",
        "\n",
        "(Make sure to enter the access token you've created before)"
      ],
      "metadata": {
        "id": "vOudnN-VA9NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "0cfaf0fd4f364dc68d507ed6e8f509d3",
            "d9cb15d1bbdd4b388535abd38b5cc822",
            "d5c35514ac3f4f1f88338b3f95045b2c",
            "82b98c555088486fae5b28caf49fa7f8",
            "7b2cd2e63d9d42f08ab030c0b7d3c95e",
            "2fc23ca708df4ad3838f6b209f99e6e8",
            "1ac2a93e110447d5816c4dce6d581ee9",
            "cfa325941c654514a2b94d340bad402c",
            "f4ca11cda8bd4cb4a321e62982d0b24a",
            "ccc44cde5d964137971b6fc38a7ed172",
            "c92ccab38bac4759999eab9ac2cf9957",
            "5f0d4434c6bf46dba63348da13695f70",
            "e182213d42754a25b249bf2b8d5c9920",
            "7555a3a105b14e5787047c5cea4f4147",
            "3eac5c3ebe1342e982bb17ae88173124",
            "4fe1d0210cbf40788e0e16c24650e7f9",
            "b08204829e31472382b08a06e4712d46",
            "e20397a8a2e042ad9e98cbe7995b3d56",
            "0ec35566847a4778aaba42dc86eff84c",
            "4e46a86acdad41d6a8e178a61c8aceda"
          ]
        },
        "id": "AqGFb4ByDA1W",
        "outputId": "e61f6cb3-ee2a-425e-917e-3162ad8ee74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cfaf0fd4f364dc68d507ed6e8f509d3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and character-level tasks\n",
        "\n",
        "LLMs don't naturally see the text on a character level, and it is one of the reasons why LLMs are not so good at arithmetics, reversing strings and similar tasks.\n",
        "\n",
        "As an example, let's investigate how LLMs and their tokenizers cope with numbers and arithmetics."
      ],
      "metadata": {
        "id": "HsiST0O_FPnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_string = '12345678'\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(initial_string)\n",
        "print('After tokenization (encoded string): ', encoded_string)\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string:\n",
        "    print(f'{token}: {encoding.decode([token])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226f5b74-b190-4d4c-c181-c738ba834cee",
        "id": "DXYS5lC59u9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After tokenization (encoded string):  [7633, 19354, 4388]\n",
            "7633: 123\n",
            "19354: 456\n",
            "4388: 78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, LLM doesn't see a large number as a sequence of characters. Instead, it operates with larger chunks of digits. This inevitable makes arithmetics harder for the LLM; just imagine the size of a \"mental multiplication table\" it should be aware of!\n",
        "\n",
        "Let's demonstrate this with an example:"
      ],
      "metadata": {
        "id": "h7tmCurKJQtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "WqCgRtIRIcN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"openai_api_key\", \"r\") as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "NRpRGdl5IdJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Nebius uses the same OpenAI() class, but with additional details\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "completion = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You're an expert in floating point computations.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"7.24*19.13 =\"\"\"\n",
        "    },\n",
        "    ],\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "completion.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YTlC-5omIVOO",
        "outputId": "6b1dc7a9-2b22-4568-c1a2-1dba330a8da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To perform this calculation, I'll use a high degree of precision to minimize rounding errors. \\n\\n7.24 × 19.13 = 138.4752\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is $138.5012$. The error is not large, but overall, don't put too much trust into LLM calculations."
      ],
      "metadata": {
        "id": "x5b0aEGFI-oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice: Further exploration of the tokenizers\n",
        "\n",
        "In this task we'll continue experimenting with LLM tokenizers to better understand their properties and restrictions.\n",
        "\n",
        "Let's make sure that we have the `tiktoken` library installed:"
      ],
      "metadata": {
        "id": "ar9xouzsMfdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMtKJTBmKScJ",
        "outputId": "e63f812e-eb76-4ddd-90ed-d91acdc4d938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by looking at some particular tokens.\n",
        "\n",
        "**Task 1.**. Print the subword unit (a string) corresponding to the token index 10101.\n",
        "\n",
        "Food for thought:\n",
        "Try to guess why do we have this strange string among tokens. It should have been quite frequent in the training data to appear as a token."
      ],
      "metadata": {
        "id": "R_H15pEhSwQR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKZ8L646drGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check the vocabulary size of GPT-4o. Web search can quickly give us the number 200k, but we want to be sure.\n",
        "\n",
        "If we explore the `encoding` object a bit more deeply, we will find:\n",
        "\n",
        "- `encoding._mergeable_ranks` contains regular `token:index` dictionary; its size is 199998.\n",
        "- `encoding._special_tokens` contains two special tokens:"
      ],
      "metadata": {
        "id": "fovDi3L5SGch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding._special_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sTijh6cWA9K",
        "outputId": "6ace46b2-e6b2-45d6-c9dd-b6d88564e8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<|endoftext|>': 199999, '<|endofprompt|>': 200018}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, indeed we have 200k items in the dictionary."
      ],
      "metadata": {
        "id": "ZtsGS5KBo_x5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.** LLM's dictionary, that is the list of all subwords it uses as tokens, is trained on common crawl web data. How exactly it is done, you'll learn further in the course. Right now, we'll observe how the training set composition influences the behaviour of tokenization for different languages.\n",
        "\n",
        "For this task, we've chosen four languages: English, French, Swahili, and Traditional Chinese. We generated a short story in Simple English about peasants and animals, and then we translated it to other languages using Google translate. Thus we've got a dictionary of four strings:\n",
        "```\n",
        "excerpts = {\n",
        "    'English': ...,\n",
        "    'French': ...,\n",
        "    'Swahili': ...,\n",
        "    'Chinese': ...\n",
        "}\n",
        "```\n",
        "\n",
        "Find the length of tokenized sequences for each of these strings. Observe the difference.\n",
        "\n",
        "**Food for thought.** Why did we choose such a naive topic for the short story? Why not adventures of a Little Brave Python in the world of Machine Learning? Why not just take the wikipedia article about Jacques-Louis David? Hint: that would damage the purity of our experiment.\n",
        "\n",
        "The data is:"
      ],
      "metadata": {
        "id": "J_0dIvz-O3yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excerpts = {\n",
        "    'English': '''\n",
        "Once upon a time, in a small village, there lived some kind peasants. They worked hard every day, taking care of their fields and animals. The peasants had cows, chickens, goats, and a big brown dog named Buddy.\n",
        "\n",
        "Every morning, the peasants woke up early. They fed the chickens, who clucked happily. They milked the cows, who gave them fresh, sweet milk. The goats liked to jump and play, making everyone laugh. Buddy, the dog, helped keep the animals safe.\n",
        "\n",
        "One day, a storm came. The wind blew hard, and the rain fell heavily. The peasants quickly brought all the animals into the big barn. Inside, it was warm and dry. The cows lay down on the soft hay. The chickens snuggled together on their perch. The goats found a corner to rest, and Buddy lay by the door, watching over everyone.\n",
        "\n",
        "The storm lasted all night, but the peasants and their animals were safe in the barn. In the morning, the sun came out, and the storm was gone. The peasants opened the barn doors, and the animals went outside, happy and free.\n",
        "\n",
        "The peasants fixed the fences and cleaned the barn. They worked together, helping each other and their animals. At the end of the day, they sat under a big tree, watching the sunset. Buddy lay at their feet, and the animals grazed nearby.\n",
        "\n",
        "The village was peaceful, and the peasants were happy. They knew that as long as they took care of their animals and each other, they would always have a good life.\n",
        "\n",
        "And so, they lived happily ever after.\n",
        "''',\n",
        "    'French': '''\n",
        "Il était une fois, dans un petit village, de gentils paysans. Ils travaillaient dur chaque jour, prenant soin de leurs champs et de leurs animaux. Les paysans avaient des vaches, des poules, des chèvres et un gros chien brun nommé Buddy.\n",
        "\n",
        "Chaque matin, les paysans se réveillaient tôt. Ils nourrissaient les poules qui glousaient joyeusement. Ils traitaient les vaches qui leur donnaient du lait frais et sucré. Les chèvres aimaient sauter et jouer, faisant rire tout le monde. Buddy, le chien, a aidé à assurer la sécurité des animaux.\n",
        "\n",
        "Un jour, une tempête est arrivée. Le vent soufflait fort et la pluie tombait abondamment. Les paysans ont rapidement amené tous les animaux dans la grande grange. À l’intérieur, il faisait chaud et sec. Les vaches se couchent sur le foin moelleux. Les poules se blottissaient les unes contre les autres sur leur perchoir. Les chèvres trouvèrent un coin pour se reposer et Buddy resta allongé près de la porte, veillant sur tout le monde.\n",
        "\n",
        "La tempête a duré toute la nuit, mais les paysans et leurs animaux étaient en sécurité dans la grange. Le matin, le soleil s'est levé et la tempête s'est calmée. Les paysans ouvrirent les portes de la grange et les animaux sortirent, heureux et libres.\n",
        "\n",
        "Les paysans réparèrent les clôtures et nettoyèrent la grange. Ils ont travaillé ensemble, s'entraidant ainsi que leurs animaux. À la fin de la journée, ils étaient assis sous un grand arbre et regardaient le coucher du soleil. Buddy gisait à leurs pieds et les animaux paissaient à proximité.\n",
        "\n",
        "Le village était paisible et les paysans étaient heureux. Ils savaient que tant qu’ils prendraient soin de leurs animaux et les uns des autres, ils auraient toujours une belle vie.\n",
        "\n",
        "Et ainsi, ils vécurent heureux pour toujours.\n",
        "''',\n",
        "  'Swahili': '''\n",
        "Hapo zamani za kale, katika kijiji kidogo waliishi wakulima wa aina fulani. Walifanya kazi kwa bidii kila siku, wakitunza mashamba na wanyama wao. Wakulima walikuwa na ng'ombe, kuku, mbuzi, na mbwa mkubwa wa kahawia aliyeitwa Buddy.\n",
        "\n",
        "Kila asubuhi, wakulima waliamka mapema. Walilisha kuku, ambao waliruka kwa furaha. Wakakamua ng'ombe, ambaye aliwapa maziwa safi, matamu. Mbuzi walipenda kuruka na kucheza, na kufanya kila mtu acheke. Buddy, mbwa, alisaidia kuweka wanyama salama.\n",
        "\n",
        "Siku moja, dhoruba ilikuja. Upepo ulivuma kwa nguvu, na mvua ikanyesha sana. Wakulima haraka wakaleta wanyama wote kwenye zizi kubwa. Ndani, kulikuwa na joto na kavu. Ng'ombe walilala kwenye nyasi laini. Kuku walijibana kwenye sangara wao. Mbuzi walipata kona ya kupumzika, na Buddy akalala karibu na mlango, akiangalia kila mtu.\n",
        "\n",
        "Dhoruba hiyo ilidumu usiku kucha, lakini wakulima na wanyama wao walikuwa salama ghalani. Asubuhi, jua lilitoka, na dhoruba ikatoweka. Wakulima walifungua milango ya ghalani, na wanyama wakatoka nje, wakiwa na furaha na huru.\n",
        "\n",
        "Wakulima walitengeneza ua na kusafisha ghala. Walifanya kazi pamoja, kusaidiana na wanyama wao. Mwisho wa siku walikaa chini ya mti mkubwa wakitazama machweo ya jua. Rafiki alilala miguuni mwao, na wanyama walilisha karibu.\n",
        "\n",
        "Kijiji kilikuwa na amani, na wakulima walikuwa na furaha. Walijua kwamba maadamu wanatunza wanyama wao na kila mmoja wao, wangekuwa na maisha mazuri kila wakati.\n",
        "\n",
        "Na hivyo, waliishi kwa furaha milele.\n",
        "''',\n",
        "  'Chinese': '''\n",
        "從前，在一個小村莊裡，住著一些善良的農夫。他們每天努力工作，照顧他們的田地和動物。農民養了牛、雞、山羊和一隻名叫巴迪的棕色大狗。\n",
        "\n",
        "每天早上，農民們都早早起床。他們餵雞，雞高興地咯咯叫。他們給乳牛擠奶，乳牛給它們新鮮、甜的牛奶。山羊們喜歡跳來跳去，玩耍，逗得大家哈哈大笑。巴迪狗幫助保護動物的安全。\n",
        "\n",
        "有一天，一場暴風雨來了。風刮得很大，雨也下得很大。農夫很快就把所有的牲畜都帶進了大穀倉。裡面溫暖而乾燥。奶牛躺在柔軟的乾草上。雞們在棲木上依偎在一起。山羊們找到了一個角落休息，巴迪躺在門口，看著大家。\n",
        "\n",
        "暴風雨持續了一夜，但農民和他們的動物在穀倉裡很安全。早上，太陽出來了，暴風雨也過去了。農民打開穀倉的門，動物們快樂自由地走出去。\n",
        "\n",
        "農民們修好了柵欄，打掃了穀倉。他們一起工作，互相幫助，也幫助他們的動物。一天結束時，他們坐在一棵大樹下，看著日落。巴迪躺在他們腳邊，動物們在附近吃草。\n",
        "\n",
        "村子裡太平了，農夫們都幸福了。他們知道，只要照顧好自己的動物和彼此，他們就會永遠過著美好的生活。\n",
        "\n",
        "就這樣，他們從此過上了幸福的生活。\n",
        "'''\n",
        "}"
      ],
      "metadata": {
        "id": "5VFqB-ANO3PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As another useful illustration, let's plot histograms of character lengths of individual tokens for each of the languages:"
      ],
      "metadata": {
        "id": "07EiZtIe3BkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_token_length_histogram(text, language_name):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-4o')\n",
        "    tokens = encoding.encode(text)\n",
        "\n",
        "    decoded_tokens = [encoding.decode([t]) for t in tokens]\n",
        "    token_lengths = [len(t) for t in decoded_tokens]\n",
        "    plt.hist(token_lengths, bins=10)\n",
        "    plt.title(f'Token Length Distribution for {language_name}')\n",
        "    plt.xlim([0,15])\n",
        "    plt.ylim([0,200])\n",
        "\n",
        "def plot_word_length_histogram(text, language_name):\n",
        "    if language != 'Chinese':\n",
        "        word_lengths = [len(t) for t in text.split()]\n",
        "        plt.hist(word_lengths, bins=10)\n",
        "        plt.title(f'Word Length Distribution for {language_name}')\n",
        "        plt.xlim([0,15])\n",
        "        plt.ylim([0,75])\n",
        "\n",
        "plt.figure(figsize=(20, 9))\n",
        "\n",
        "for i, language in enumerate(['English', 'French', 'Swahili', 'Chinese']):\n",
        "    # Token length histograms\n",
        "    plt.subplot(2, 4, 1 + i)\n",
        "    plot_token_length_histogram(excerpts[language], language)\n",
        "\n",
        "    # Word length histograms\n",
        "    plt.subplot(2, 4, 5 + i)\n",
        "    plot_word_length_histogram(excerpts[language], language)\n"
      ],
      "metadata": {
        "id": "57uLjAolOHZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which peculiarities do you observe?"
      ],
      "metadata": {
        "id": "bWqKis9d5F1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.** Check how the tokenizer splits the Chinese text. You will find that some of the tokens are strange and have no clear correspondence in the text. Why do we have such tokens?"
      ],
      "metadata": {
        "id": "Mrv8cTKar4x4"
      }
    }
  ]
}